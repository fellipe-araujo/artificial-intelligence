{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer, TfidfTransformer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.metrics import f1_score\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.corpus import stopwords\n",
    "from tabulate import tabulate\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  themes process_id                            file_name document_type  pages  \\\n",
      "0  [232]  AI_856934  AI_856934_1926210_1060_17072013.pdf        outros      1   \n",
      "1  [232]  AI_856934    AI_856934_1926211_34_17072013.pdf        outros      1   \n",
      "2  [232]  AI_856934    AI_856934_1926211_34_17072013.pdf        outros      2   \n",
      "3  [232]  AI_856934    AI_856934_1926211_34_17072013.pdf        outros      3   \n",
      "4  [232]  AI_856934    AI_856934_1926211_34_17072013.pdf        outros      4   \n",
      "\n",
      "                                                body  \n",
      "0  {\"tribunal justiça estado bahia poder judiciár...  \n",
      "1  {\"excelentíssimo senhor doutor juiz direito ju...  \n",
      "2  {\"razões recurso inominado recorrente atlantic...  \n",
      "3  {\"empresa recorrente tornou credora dos débito...  \n",
      "4  {\"entretanto verdade parte apelante tornou tit...  \n"
     ]
    }
   ],
   "source": [
    "file_train = pd.read_csv('data_files/train_small.csv')\n",
    "file_test = pd.read_csv('data_files/test_small.csv')\n",
    "\n",
    "print(file_train[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    {\"tribunal justiça estado bahia poder judiciár...\n",
      "1    {\"excelentíssimo senhor doutor juiz direito ju...\n",
      "2    {\"razões recurso inominado recorrente atlantic...\n",
      "3    {\"empresa recorrente tornou credora dos débito...\n",
      "4    {\"entretanto verdade parte apelante tornou tit...\n",
      "Name: body, dtype: object\n",
      "0    outros\n",
      "1    outros\n",
      "2    outros\n",
      "3    outros\n",
      "4    outros\n",
      "Name: document_type, dtype: object\n"
     ]
    }
   ],
   "source": [
    "x_train = file_train['body']\n",
    "y_train = file_train['document_type']\n",
    "\n",
    "x_test = file_test['body']\n",
    "y_test = file_test['document_type']\n",
    "\n",
    "print(x_train[:5])\n",
    "print(y_train[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "porter_stemmer = PorterStemmer()\n",
    "\n",
    "def stemming_tokenizer(str_input):\n",
    "  words = re.sub(r\"[^A-Za-z0-9\\-]\", \" \", str_input).lower().split()\n",
    "  words = [porter_stemmer.stem(word) for word in words]\n",
    "  return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 16 candidates, totalling 80 fits\n",
      "[CV 1/5] END clf__alpha=0.01, tfidf__norm=l1, vect__max_df=0.5, vect__ngram_range=(1, 2), vect__tokenizer=<function stemming_tokenizer at 0x7fc0e07d7a60>; accuracy: (test=0.899) f1_macro: (test=0.158) f1_micro: (test=0.899) f1_weighted: (test=0.851) total time= 9.3min\n",
      "[CV 2/5] END clf__alpha=0.01, tfidf__norm=l1, vect__max_df=0.5, vect__ngram_range=(1, 2), vect__tokenizer=<function stemming_tokenizer at 0x7fc0e07d7a60>; accuracy: (test=0.899) f1_macro: (test=0.158) f1_micro: (test=0.899) f1_weighted: (test=0.851) total time=12.5min\n",
      "[CV 3/5] END clf__alpha=0.01, tfidf__norm=l1, vect__max_df=0.5, vect__ngram_range=(1, 2), vect__tokenizer=<function stemming_tokenizer at 0x7fc0e07d7a60>; accuracy: (test=0.899) f1_macro: (test=0.158) f1_micro: (test=0.899) f1_weighted: (test=0.851) total time=12.8min\n",
      "[CV 4/5] END clf__alpha=0.01, tfidf__norm=l1, vect__max_df=0.5, vect__ngram_range=(1, 2), vect__tokenizer=<function stemming_tokenizer at 0x7fc0e07d7a60>; accuracy: (test=0.899) f1_macro: (test=0.158) f1_micro: (test=0.899) f1_weighted: (test=0.851) total time=12.7min\n",
      "[CV 5/5] END clf__alpha=0.01, tfidf__norm=l1, vect__max_df=0.5, vect__ngram_range=(1, 2), vect__tokenizer=<function stemming_tokenizer at 0x7fc0e07d7a60>; accuracy: (test=0.899) f1_macro: (test=0.158) f1_micro: (test=0.899) f1_weighted: (test=0.851) total time=12.6min\n",
      "[CV 1/5] END clf__alpha=0.01, tfidf__norm=l1, vect__max_df=0.5, vect__ngram_range=(1, 3), vect__tokenizer=<function stemming_tokenizer at 0x7fc0e07d7a60>; accuracy: (test=0.899) f1_macro: (test=0.158) f1_micro: (test=0.899) f1_weighted: (test=0.851) total time=13.8min\n",
      "[CV 2/5] END clf__alpha=0.01, tfidf__norm=l1, vect__max_df=0.5, vect__ngram_range=(1, 3), vect__tokenizer=<function stemming_tokenizer at 0x7fc0e07d7a60>; accuracy: (test=0.899) f1_macro: (test=0.158) f1_micro: (test=0.899) f1_weighted: (test=0.851) total time=13.9min\n",
      "[CV 3/5] END clf__alpha=0.01, tfidf__norm=l1, vect__max_df=0.5, vect__ngram_range=(1, 3), vect__tokenizer=<function stemming_tokenizer at 0x7fc0e07d7a60>; accuracy: (test=0.899) f1_macro: (test=0.158) f1_micro: (test=0.899) f1_weighted: (test=0.851) total time=14.0min\n",
      "[CV 4/5] END clf__alpha=0.01, tfidf__norm=l1, vect__max_df=0.5, vect__ngram_range=(1, 3), vect__tokenizer=<function stemming_tokenizer at 0x7fc0e07d7a60>; accuracy: (test=0.899) f1_macro: (test=0.158) f1_micro: (test=0.899) f1_weighted: (test=0.851) total time=13.9min\n",
      "[CV 5/5] END clf__alpha=0.01, tfidf__norm=l1, vect__max_df=0.5, vect__ngram_range=(1, 3), vect__tokenizer=<function stemming_tokenizer at 0x7fc0e07d7a60>; accuracy: (test=0.899) f1_macro: (test=0.158) f1_micro: (test=0.899) f1_weighted: (test=0.851) total time=13.8min\n",
      "[CV 1/5] END clf__alpha=0.01, tfidf__norm=l1, vect__max_df=0.75, vect__ngram_range=(1, 2), vect__tokenizer=<function stemming_tokenizer at 0x7fc0e07d7a60>; accuracy: (test=0.899) f1_macro: (test=0.158) f1_micro: (test=0.899) f1_weighted: (test=0.851) total time=12.6min\n",
      "[CV 2/5] END clf__alpha=0.01, tfidf__norm=l1, vect__max_df=0.75, vect__ngram_range=(1, 2), vect__tokenizer=<function stemming_tokenizer at 0x7fc0e07d7a60>; accuracy: (test=0.899) f1_macro: (test=0.158) f1_micro: (test=0.899) f1_weighted: (test=0.851) total time=12.7min\n",
      "[CV 3/5] END clf__alpha=0.01, tfidf__norm=l1, vect__max_df=0.75, vect__ngram_range=(1, 2), vect__tokenizer=<function stemming_tokenizer at 0x7fc0e07d7a60>; accuracy: (test=0.899) f1_macro: (test=0.158) f1_micro: (test=0.899) f1_weighted: (test=0.851) total time=12.7min\n",
      "[CV 4/5] END clf__alpha=0.01, tfidf__norm=l1, vect__max_df=0.75, vect__ngram_range=(1, 2), vect__tokenizer=<function stemming_tokenizer at 0x7fc0e07d7a60>; accuracy: (test=0.899) f1_macro: (test=0.158) f1_micro: (test=0.899) f1_weighted: (test=0.851) total time=12.7min\n",
      "[CV 5/5] END clf__alpha=0.01, tfidf__norm=l1, vect__max_df=0.75, vect__ngram_range=(1, 2), vect__tokenizer=<function stemming_tokenizer at 0x7fc0e07d7a60>; accuracy: (test=0.899) f1_macro: (test=0.158) f1_micro: (test=0.899) f1_weighted: (test=0.851) total time=12.7min\n",
      "[CV 1/5] END clf__alpha=0.01, tfidf__norm=l1, vect__max_df=0.75, vect__ngram_range=(1, 3), vect__tokenizer=<function stemming_tokenizer at 0x7fc0e07d7a60>; accuracy: (test=0.899) f1_macro: (test=0.158) f1_micro: (test=0.899) f1_weighted: (test=0.851) total time=13.9min\n",
      "[CV 2/5] END clf__alpha=0.01, tfidf__norm=l1, vect__max_df=0.75, vect__ngram_range=(1, 3), vect__tokenizer=<function stemming_tokenizer at 0x7fc0e07d7a60>; accuracy: (test=0.899) f1_macro: (test=0.158) f1_micro: (test=0.899) f1_weighted: (test=0.851) total time=14.0min\n",
      "[CV 3/5] END clf__alpha=0.01, tfidf__norm=l1, vect__max_df=0.75, vect__ngram_range=(1, 3), vect__tokenizer=<function stemming_tokenizer at 0x7fc0e07d7a60>; accuracy: (test=0.899) f1_macro: (test=0.158) f1_micro: (test=0.899) f1_weighted: (test=0.851) total time=13.9min\n",
      "[CV 4/5] END clf__alpha=0.01, tfidf__norm=l1, vect__max_df=0.75, vect__ngram_range=(1, 3), vect__tokenizer=<function stemming_tokenizer at 0x7fc0e07d7a60>; accuracy: (test=0.899) f1_macro: (test=0.158) f1_micro: (test=0.899) f1_weighted: (test=0.851) total time=13.9min\n",
      "[CV 5/5] END clf__alpha=0.01, tfidf__norm=l1, vect__max_df=0.75, vect__ngram_range=(1, 3), vect__tokenizer=<function stemming_tokenizer at 0x7fc0e07d7a60>; accuracy: (test=0.899) f1_macro: (test=0.158) f1_micro: (test=0.899) f1_weighted: (test=0.851) total time=13.9min\n",
      "[CV 1/5] END clf__alpha=0.01, tfidf__norm=l2, vect__max_df=0.5, vect__ngram_range=(1, 2), vect__tokenizer=<function stemming_tokenizer at 0x7fc0e07d7a60>; accuracy: (test=0.899) f1_macro: (test=0.158) f1_micro: (test=0.899) f1_weighted: (test=0.851) total time=12.6min\n",
      "[CV 2/5] END clf__alpha=0.01, tfidf__norm=l2, vect__max_df=0.5, vect__ngram_range=(1, 2), vect__tokenizer=<function stemming_tokenizer at 0x7fc0e07d7a60>; accuracy: (test=0.899) f1_macro: (test=0.158) f1_micro: (test=0.899) f1_weighted: (test=0.851) total time=12.7min\n",
      "[CV 3/5] END clf__alpha=0.01, tfidf__norm=l2, vect__max_df=0.5, vect__ngram_range=(1, 2), vect__tokenizer=<function stemming_tokenizer at 0x7fc0e07d7a60>; accuracy: (test=0.899) f1_macro: (test=0.158) f1_micro: (test=0.899) f1_weighted: (test=0.851) total time=13.0min\n",
      "[CV 4/5] END clf__alpha=0.01, tfidf__norm=l2, vect__max_df=0.5, vect__ngram_range=(1, 2), vect__tokenizer=<function stemming_tokenizer at 0x7fc0e07d7a60>; accuracy: (test=0.899) f1_macro: (test=0.158) f1_micro: (test=0.899) f1_weighted: (test=0.851) total time=13.6min\n",
      "[CV 5/5] END clf__alpha=0.01, tfidf__norm=l2, vect__max_df=0.5, vect__ngram_range=(1, 2), vect__tokenizer=<function stemming_tokenizer at 0x7fc0e07d7a60>; accuracy: (test=0.899) f1_macro: (test=0.158) f1_micro: (test=0.899) f1_weighted: (test=0.851) total time=13.1min\n",
      "[CV 1/5] END clf__alpha=0.01, tfidf__norm=l2, vect__max_df=0.5, vect__ngram_range=(1, 3), vect__tokenizer=<function stemming_tokenizer at 0x7fc0e07d7a60>; accuracy: (test=0.899) f1_macro: (test=0.158) f1_micro: (test=0.899) f1_weighted: (test=0.851) total time=15.1min\n",
      "[CV 2/5] END clf__alpha=0.01, tfidf__norm=l2, vect__max_df=0.5, vect__ngram_range=(1, 3), vect__tokenizer=<function stemming_tokenizer at 0x7fc0e07d7a60>; accuracy: (test=0.899) f1_macro: (test=0.158) f1_micro: (test=0.899) f1_weighted: (test=0.851) total time=14.3min\n",
      "[CV 3/5] END clf__alpha=0.01, tfidf__norm=l2, vect__max_df=0.5, vect__ngram_range=(1, 3), vect__tokenizer=<function stemming_tokenizer at 0x7fc0e07d7a60>; accuracy: (test=0.899) f1_macro: (test=0.158) f1_micro: (test=0.899) f1_weighted: (test=0.851) total time=14.1min\n",
      "[CV 4/5] END clf__alpha=0.01, tfidf__norm=l2, vect__max_df=0.5, vect__ngram_range=(1, 3), vect__tokenizer=<function stemming_tokenizer at 0x7fc0e07d7a60>; accuracy: (test=0.899) f1_macro: (test=0.158) f1_micro: (test=0.899) f1_weighted: (test=0.851) total time=14.3min\n",
      "[CV 5/5] END clf__alpha=0.01, tfidf__norm=l2, vect__max_df=0.5, vect__ngram_range=(1, 3), vect__tokenizer=<function stemming_tokenizer at 0x7fc0e07d7a60>; accuracy: (test=0.899) f1_macro: (test=0.158) f1_micro: (test=0.899) f1_weighted: (test=0.851) total time=14.3min\n",
      "[CV 1/5] END clf__alpha=0.01, tfidf__norm=l2, vect__max_df=0.75, vect__ngram_range=(1, 2), vect__tokenizer=<function stemming_tokenizer at 0x7fc0e07d7a60>; accuracy: (test=0.899) f1_macro: (test=0.158) f1_micro: (test=0.899) f1_weighted: (test=0.851) total time=12.6min\n",
      "[CV 2/5] END clf__alpha=0.01, tfidf__norm=l2, vect__max_df=0.75, vect__ngram_range=(1, 2), vect__tokenizer=<function stemming_tokenizer at 0x7fc0e07d7a60>; accuracy: (test=0.899) f1_macro: (test=0.158) f1_micro: (test=0.899) f1_weighted: (test=0.851) total time=13.2min\n",
      "[CV 3/5] END clf__alpha=0.01, tfidf__norm=l2, vect__max_df=0.75, vect__ngram_range=(1, 2), vect__tokenizer=<function stemming_tokenizer at 0x7fc0e07d7a60>; accuracy: (test=0.899) f1_macro: (test=0.158) f1_micro: (test=0.899) f1_weighted: (test=0.851) total time=12.9min\n",
      "[CV 4/5] END clf__alpha=0.01, tfidf__norm=l2, vect__max_df=0.75, vect__ngram_range=(1, 2), vect__tokenizer=<function stemming_tokenizer at 0x7fc0e07d7a60>; accuracy: (test=0.899) f1_macro: (test=0.158) f1_micro: (test=0.899) f1_weighted: (test=0.851) total time=13.6min\n",
      "[CV 5/5] END clf__alpha=0.01, tfidf__norm=l2, vect__max_df=0.75, vect__ngram_range=(1, 2), vect__tokenizer=<function stemming_tokenizer at 0x7fc0e07d7a60>; accuracy: (test=0.899) f1_macro: (test=0.158) f1_micro: (test=0.899) f1_weighted: (test=0.851) total time=13.1min\n",
      "[CV 1/5] END clf__alpha=0.01, tfidf__norm=l2, vect__max_df=0.75, vect__ngram_range=(1, 3), vect__tokenizer=<function stemming_tokenizer at 0x7fc0e07d7a60>; accuracy: (test=0.899) f1_macro: (test=0.158) f1_micro: (test=0.899) f1_weighted: (test=0.851) total time=14.8min\n",
      "[CV 2/5] END clf__alpha=0.01, tfidf__norm=l2, vect__max_df=0.75, vect__ngram_range=(1, 3), vect__tokenizer=<function stemming_tokenizer at 0x7fc0e07d7a60>; accuracy: (test=0.899) f1_macro: (test=0.158) f1_micro: (test=0.899) f1_weighted: (test=0.851) total time=14.6min\n",
      "[CV 3/5] END clf__alpha=0.01, tfidf__norm=l2, vect__max_df=0.75, vect__ngram_range=(1, 3), vect__tokenizer=<function stemming_tokenizer at 0x7fc0e07d7a60>; accuracy: (test=0.899) f1_macro: (test=0.158) f1_micro: (test=0.899) f1_weighted: (test=0.851) total time=14.3min\n",
      "[CV 4/5] END clf__alpha=0.01, tfidf__norm=l2, vect__max_df=0.75, vect__ngram_range=(1, 3), vect__tokenizer=<function stemming_tokenizer at 0x7fc0e07d7a60>; accuracy: (test=0.899) f1_macro: (test=0.158) f1_micro: (test=0.899) f1_weighted: (test=0.851) total time=14.2min\n",
      "[CV 5/5] END clf__alpha=0.01, tfidf__norm=l2, vect__max_df=0.75, vect__ngram_range=(1, 3), vect__tokenizer=<function stemming_tokenizer at 0x7fc0e07d7a60>; accuracy: (test=0.899) f1_macro: (test=0.158) f1_micro: (test=0.899) f1_weighted: (test=0.851) total time=14.2min\n",
      "[CV 1/5] END clf__alpha=0.001, tfidf__norm=l1, vect__max_df=0.5, vect__ngram_range=(1, 2), vect__tokenizer=<function stemming_tokenizer at 0x7fc0e07d7a60>; accuracy: (test=0.899) f1_macro: (test=0.158) f1_micro: (test=0.899) f1_weighted: (test=0.851) total time=12.5min\n",
      "[CV 2/5] END clf__alpha=0.001, tfidf__norm=l1, vect__max_df=0.5, vect__ngram_range=(1, 2), vect__tokenizer=<function stemming_tokenizer at 0x7fc0e07d7a60>; accuracy: (test=0.899) f1_macro: (test=0.158) f1_micro: (test=0.899) f1_weighted: (test=0.851) total time=12.4min\n",
      "[CV 3/5] END clf__alpha=0.001, tfidf__norm=l1, vect__max_df=0.5, vect__ngram_range=(1, 2), vect__tokenizer=<function stemming_tokenizer at 0x7fc0e07d7a60>; accuracy: (test=0.899) f1_macro: (test=0.158) f1_micro: (test=0.899) f1_weighted: (test=0.851) total time=12.4min\n",
      "[CV 4/5] END clf__alpha=0.001, tfidf__norm=l1, vect__max_df=0.5, vect__ngram_range=(1, 2), vect__tokenizer=<function stemming_tokenizer at 0x7fc0e07d7a60>; accuracy: (test=0.899) f1_macro: (test=0.158) f1_micro: (test=0.899) f1_weighted: (test=0.851) total time=12.3min\n",
      "[CV 5/5] END clf__alpha=0.001, tfidf__norm=l1, vect__max_df=0.5, vect__ngram_range=(1, 2), vect__tokenizer=<function stemming_tokenizer at 0x7fc0e07d7a60>; accuracy: (test=0.899) f1_macro: (test=0.158) f1_micro: (test=0.899) f1_weighted: (test=0.851) total time=12.3min\n",
      "[CV 1/5] END clf__alpha=0.001, tfidf__norm=l1, vect__max_df=0.5, vect__ngram_range=(1, 3), vect__tokenizer=<function stemming_tokenizer at 0x7fc0e07d7a60>; accuracy: (test=0.899) f1_macro: (test=0.158) f1_micro: (test=0.899) f1_weighted: (test=0.851) total time=13.4min\n",
      "[CV 2/5] END clf__alpha=0.001, tfidf__norm=l1, vect__max_df=0.5, vect__ngram_range=(1, 3), vect__tokenizer=<function stemming_tokenizer at 0x7fc0e07d7a60>; accuracy: (test=0.899) f1_macro: (test=0.158) f1_micro: (test=0.899) f1_weighted: (test=0.851) total time=13.6min\n",
      "[CV 3/5] END clf__alpha=0.001, tfidf__norm=l1, vect__max_df=0.5, vect__ngram_range=(1, 3), vect__tokenizer=<function stemming_tokenizer at 0x7fc0e07d7a60>; accuracy: (test=0.899) f1_macro: (test=0.158) f1_micro: (test=0.899) f1_weighted: (test=0.851) total time=13.5min\n",
      "[CV 4/5] END clf__alpha=0.001, tfidf__norm=l1, vect__max_df=0.5, vect__ngram_range=(1, 3), vect__tokenizer=<function stemming_tokenizer at 0x7fc0e07d7a60>; accuracy: (test=0.899) f1_macro: (test=0.158) f1_micro: (test=0.899) f1_weighted: (test=0.851) total time=13.6min\n",
      "[CV 5/5] END clf__alpha=0.001, tfidf__norm=l1, vect__max_df=0.5, vect__ngram_range=(1, 3), vect__tokenizer=<function stemming_tokenizer at 0x7fc0e07d7a60>; accuracy: (test=0.899) f1_macro: (test=0.158) f1_micro: (test=0.899) f1_weighted: (test=0.851) total time=13.6min\n",
      "[CV 1/5] END clf__alpha=0.001, tfidf__norm=l1, vect__max_df=0.75, vect__ngram_range=(1, 2), vect__tokenizer=<function stemming_tokenizer at 0x7fc0e07d7a60>; accuracy: (test=0.899) f1_macro: (test=0.158) f1_micro: (test=0.899) f1_weighted: (test=0.851) total time=12.4min\n",
      "[CV 2/5] END clf__alpha=0.001, tfidf__norm=l1, vect__max_df=0.75, vect__ngram_range=(1, 2), vect__tokenizer=<function stemming_tokenizer at 0x7fc0e07d7a60>; accuracy: (test=0.899) f1_macro: (test=0.158) f1_micro: (test=0.899) f1_weighted: (test=0.851) total time=12.4min\n",
      "[CV 3/5] END clf__alpha=0.001, tfidf__norm=l1, vect__max_df=0.75, vect__ngram_range=(1, 2), vect__tokenizer=<function stemming_tokenizer at 0x7fc0e07d7a60>; accuracy: (test=0.899) f1_macro: (test=0.158) f1_micro: (test=0.899) f1_weighted: (test=0.851) total time=12.4min\n",
      "[CV 4/5] END clf__alpha=0.001, tfidf__norm=l1, vect__max_df=0.75, vect__ngram_range=(1, 2), vect__tokenizer=<function stemming_tokenizer at 0x7fc0e07d7a60>; accuracy: (test=0.899) f1_macro: (test=0.158) f1_micro: (test=0.899) f1_weighted: (test=0.851) total time=12.4min\n",
      "[CV 5/5] END clf__alpha=0.001, tfidf__norm=l1, vect__max_df=0.75, vect__ngram_range=(1, 2), vect__tokenizer=<function stemming_tokenizer at 0x7fc0e07d7a60>; accuracy: (test=0.899) f1_macro: (test=0.158) f1_micro: (test=0.899) f1_weighted: (test=0.851) total time=12.3min\n",
      "[CV 1/5] END clf__alpha=0.001, tfidf__norm=l1, vect__max_df=0.75, vect__ngram_range=(1, 3), vect__tokenizer=<function stemming_tokenizer at 0x7fc0e07d7a60>; accuracy: (test=0.899) f1_macro: (test=0.158) f1_micro: (test=0.899) f1_weighted: (test=0.851) total time=13.6min\n",
      "[CV 2/5] END clf__alpha=0.001, tfidf__norm=l1, vect__max_df=0.75, vect__ngram_range=(1, 3), vect__tokenizer=<function stemming_tokenizer at 0x7fc0e07d7a60>; accuracy: (test=0.899) f1_macro: (test=0.158) f1_micro: (test=0.899) f1_weighted: (test=0.851) total time=13.6min\n",
      "[CV 3/5] END clf__alpha=0.001, tfidf__norm=l1, vect__max_df=0.75, vect__ngram_range=(1, 3), vect__tokenizer=<function stemming_tokenizer at 0x7fc0e07d7a60>; accuracy: (test=0.899) f1_macro: (test=0.158) f1_micro: (test=0.899) f1_weighted: (test=0.851) total time=13.5min\n",
      "[CV 4/5] END clf__alpha=0.001, tfidf__norm=l1, vect__max_df=0.75, vect__ngram_range=(1, 3), vect__tokenizer=<function stemming_tokenizer at 0x7fc0e07d7a60>; accuracy: (test=0.899) f1_macro: (test=0.158) f1_micro: (test=0.899) f1_weighted: (test=0.851) total time=13.5min\n",
      "[CV 5/5] END clf__alpha=0.001, tfidf__norm=l1, vect__max_df=0.75, vect__ngram_range=(1, 3), vect__tokenizer=<function stemming_tokenizer at 0x7fc0e07d7a60>; accuracy: (test=0.899) f1_macro: (test=0.158) f1_micro: (test=0.899) f1_weighted: (test=0.851) total time=13.5min\n",
      "[CV 1/5] END clf__alpha=0.001, tfidf__norm=l2, vect__max_df=0.5, vect__ngram_range=(1, 2), vect__tokenizer=<function stemming_tokenizer at 0x7fc0e07d7a60>; accuracy: (test=0.899) f1_macro: (test=0.158) f1_micro: (test=0.899) f1_weighted: (test=0.851) total time=12.4min\n",
      "[CV 2/5] END clf__alpha=0.001, tfidf__norm=l2, vect__max_df=0.5, vect__ngram_range=(1, 2), vect__tokenizer=<function stemming_tokenizer at 0x7fc0e07d7a60>; accuracy: (test=0.899) f1_macro: (test=0.158) f1_micro: (test=0.899) f1_weighted: (test=0.851) total time=12.4min\n",
      "[CV 3/5] END clf__alpha=0.001, tfidf__norm=l2, vect__max_df=0.5, vect__ngram_range=(1, 2), vect__tokenizer=<function stemming_tokenizer at 0x7fc0e07d7a60>; accuracy: (test=0.899) f1_macro: (test=0.158) f1_micro: (test=0.899) f1_weighted: (test=0.851) total time=12.4min\n",
      "[CV 4/5] END clf__alpha=0.001, tfidf__norm=l2, vect__max_df=0.5, vect__ngram_range=(1, 2), vect__tokenizer=<function stemming_tokenizer at 0x7fc0e07d7a60>; accuracy: (test=0.899) f1_macro: (test=0.158) f1_micro: (test=0.899) f1_weighted: (test=0.851) total time=12.3min\n",
      "[CV 5/5] END clf__alpha=0.001, tfidf__norm=l2, vect__max_df=0.5, vect__ngram_range=(1, 2), vect__tokenizer=<function stemming_tokenizer at 0x7fc0e07d7a60>; accuracy: (test=0.899) f1_macro: (test=0.158) f1_micro: (test=0.899) f1_weighted: (test=0.851) total time=12.3min\n",
      "[CV 1/5] END clf__alpha=0.001, tfidf__norm=l2, vect__max_df=0.5, vect__ngram_range=(1, 3), vect__tokenizer=<function stemming_tokenizer at 0x7fc0e07d7a60>; accuracy: (test=0.899) f1_macro: (test=0.158) f1_micro: (test=0.899) f1_weighted: (test=0.851) total time=13.5min\n",
      "[CV 2/5] END clf__alpha=0.001, tfidf__norm=l2, vect__max_df=0.5, vect__ngram_range=(1, 3), vect__tokenizer=<function stemming_tokenizer at 0x7fc0e07d7a60>; accuracy: (test=0.899) f1_macro: (test=0.158) f1_micro: (test=0.899) f1_weighted: (test=0.851) total time=13.6min\n",
      "[CV 3/5] END clf__alpha=0.001, tfidf__norm=l2, vect__max_df=0.5, vect__ngram_range=(1, 3), vect__tokenizer=<function stemming_tokenizer at 0x7fc0e07d7a60>; accuracy: (test=0.899) f1_macro: (test=0.158) f1_micro: (test=0.899) f1_weighted: (test=0.851) total time=13.6min\n",
      "[CV 4/5] END clf__alpha=0.001, tfidf__norm=l2, vect__max_df=0.5, vect__ngram_range=(1, 3), vect__tokenizer=<function stemming_tokenizer at 0x7fc0e07d7a60>; accuracy: (test=0.899) f1_macro: (test=0.158) f1_micro: (test=0.899) f1_weighted: (test=0.851) total time=13.6min\n",
      "[CV 5/5] END clf__alpha=0.001, tfidf__norm=l2, vect__max_df=0.5, vect__ngram_range=(1, 3), vect__tokenizer=<function stemming_tokenizer at 0x7fc0e07d7a60>; accuracy: (test=0.899) f1_macro: (test=0.158) f1_micro: (test=0.899) f1_weighted: (test=0.851) total time=13.6min\n",
      "[CV 1/5] END clf__alpha=0.001, tfidf__norm=l2, vect__max_df=0.75, vect__ngram_range=(1, 2), vect__tokenizer=<function stemming_tokenizer at 0x7fc0e07d7a60>; accuracy: (test=0.899) f1_macro: (test=0.158) f1_micro: (test=0.899) f1_weighted: (test=0.851) total time=12.4min\n",
      "[CV 2/5] END clf__alpha=0.001, tfidf__norm=l2, vect__max_df=0.75, vect__ngram_range=(1, 2), vect__tokenizer=<function stemming_tokenizer at 0x7fc0e07d7a60>; accuracy: (test=0.899) f1_macro: (test=0.158) f1_micro: (test=0.899) f1_weighted: (test=0.851) total time=12.4min\n",
      "[CV 3/5] END clf__alpha=0.001, tfidf__norm=l2, vect__max_df=0.75, vect__ngram_range=(1, 2), vect__tokenizer=<function stemming_tokenizer at 0x7fc0e07d7a60>; accuracy: (test=0.899) f1_macro: (test=0.158) f1_micro: (test=0.899) f1_weighted: (test=0.851) total time=12.4min\n",
      "[CV 4/5] END clf__alpha=0.001, tfidf__norm=l2, vect__max_df=0.75, vect__ngram_range=(1, 2), vect__tokenizer=<function stemming_tokenizer at 0x7fc0e07d7a60>; accuracy: (test=0.899) f1_macro: (test=0.158) f1_micro: (test=0.899) f1_weighted: (test=0.851) total time=12.4min\n",
      "[CV 5/5] END clf__alpha=0.001, tfidf__norm=l2, vect__max_df=0.75, vect__ngram_range=(1, 2), vect__tokenizer=<function stemming_tokenizer at 0x7fc0e07d7a60>; accuracy: (test=0.899) f1_macro: (test=0.158) f1_micro: (test=0.899) f1_weighted: (test=0.851) total time=12.3min\n",
      "[CV 1/5] END clf__alpha=0.001, tfidf__norm=l2, vect__max_df=0.75, vect__ngram_range=(1, 3), vect__tokenizer=<function stemming_tokenizer at 0x7fc0e07d7a60>; accuracy: (test=0.899) f1_macro: (test=0.158) f1_micro: (test=0.899) f1_weighted: (test=0.851) total time=13.5min\n",
      "[CV 2/5] END clf__alpha=0.001, tfidf__norm=l2, vect__max_df=0.75, vect__ngram_range=(1, 3), vect__tokenizer=<function stemming_tokenizer at 0x7fc0e07d7a60>; accuracy: (test=0.899) f1_macro: (test=0.158) f1_micro: (test=0.899) f1_weighted: (test=0.851) total time=13.6min\n",
      "[CV 3/5] END clf__alpha=0.001, tfidf__norm=l2, vect__max_df=0.75, vect__ngram_range=(1, 3), vect__tokenizer=<function stemming_tokenizer at 0x7fc0e07d7a60>; accuracy: (test=0.899) f1_macro: (test=0.158) f1_micro: (test=0.899) f1_weighted: (test=0.851) total time=13.6min\n",
      "[CV 4/5] END clf__alpha=0.001, tfidf__norm=l2, vect__max_df=0.75, vect__ngram_range=(1, 3), vect__tokenizer=<function stemming_tokenizer at 0x7fc0e07d7a60>; accuracy: (test=0.899) f1_macro: (test=0.158) f1_micro: (test=0.899) f1_weighted: (test=0.851) total time=13.6min\n",
      "[CV 5/5] END clf__alpha=0.001, tfidf__norm=l2, vect__max_df=0.75, vect__ngram_range=(1, 3), vect__tokenizer=<function stemming_tokenizer at 0x7fc0e07d7a60>; accuracy: (test=0.899) f1_macro: (test=0.158) f1_micro: (test=0.899) f1_weighted: (test=0.851) total time=13.6min\n",
      "Best parameters: {'clf__alpha': 0.01, 'tfidf__norm': 'l1', 'vect__max_df': 0.5, 'vect__ngram_range': (1, 2), 'vect__tokenizer': <function stemming_tokenizer at 0x7fc0e07d7a60>}\n",
      "Best score: 0.8989190241242083\n"
     ]
    }
   ],
   "source": [
    "pipeline = Pipeline([\n",
    "  ('vect', CountVectorizer()),\n",
    "  ('tfidf', TfidfTransformer()),\n",
    "  ('clf', SGDClassifier()),\n",
    "])\n",
    "\n",
    "parameters = {\n",
    "  'vect__max_df': (0.5, 0.75),\n",
    "  'vect__ngram_range': ((1, 2), (1, 3)), # unigrams or bigrams or trigrams\n",
    "  'vect__tokenizer': ([stemming_tokenizer]),\n",
    "  'tfidf__norm': ('l1', 'l2'),\n",
    "  'clf__alpha': (0.01, 0.001),\n",
    "}\n",
    "\n",
    "scoring = {\n",
    "  'accuracy',\n",
    "  'f1_micro',\n",
    "  'f1_macro',\n",
    "  'f1_weighted'\n",
    "}\n",
    "\n",
    "grid_search = GridSearchCV(estimator=pipeline, param_grid=parameters, n_jobs=None, verbose=3, return_train_score=False, scoring=scoring, refit='f1_micro')\n",
    "\n",
    "grid_search.fit(x_train, y_train)\n",
    "\n",
    "print('Best parameters: {}'.format(grid_search.best_params_))\n",
    "print('Best score: {}'.format(grid_search.best_score_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 Score (micro): 0.8940811925549065\n"
     ]
    }
   ],
   "source": [
    "best_estimator = grid_search.best_estimator_\n",
    "\n",
    "print('F1 Score (micro): {}'.format(f1_score(y_test, best_estimator.predict(x_test), average='micro')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "╒═════════════════════════════════╤════════════════╤═════════════════════╤════════════════════╤═══════════════╤══════════════╤══════════════╤════════════════════╕\n",
      "│ Classifiers                     │   vect__max_df │ vect__ngram_range   │ vect__tokenizer    │ tfidf__norm   │   clf__alpha │   Best Score │   F1 Score - micro │\n",
      "╞═════════════════════════════════╪════════════════╪═════════════════════╪════════════════════╪═══════════════╪══════════════╪══════════════╪════════════════════╡\n",
      "│ CountVectorizer + SGDClassifier │            0.5 │ (1, 2)              │ stemming_tokenizer │ l1            │         0.01 │     0.898919 │           0.894081 │\n",
      "╘═════════════════════════════════╧════════════════╧═════════════════════╧════════════════════╧═══════════════╧══════════════╧══════════════╧════════════════════╛\n"
     ]
    }
   ],
   "source": [
    "header = ['Classifiers', 'vect__max_df', 'vect__ngram_range', 'vect__tokenizer', 'tfidf__norm', 'clf__alpha', 'Best Score', 'F1 Score - micro']\n",
    "\n",
    "metrics = []\n",
    "\n",
    "metrics.append(['CountVectorizer + SGDClassifier', '0.5', '(1, 2)', 'stemming_tokenizer', 'l1', '0.01', '0.898919', '0.894081'])\n",
    "\n",
    "print(tabulate(metrics, headers=header, tablefmt=\"fancy_grid\"))"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
  },
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
